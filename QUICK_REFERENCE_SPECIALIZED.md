# üéØ Quick Reference: Specialized Layer Architecture

## **TL;DR: What Changed?**

Each layer now uses **specialized models and prompts** instead of generic ones.

---

## üìã Layer Quick Reference

| Layer | Model | Purpose | Speed | When Used |
|-------|-------|---------|-------|-----------|
| **Perception** | MiniLM + BART | Understand input | ‚ö°‚ö°‚ö° | Every query |
| **Memory** | MongoDB | Store/retrieve data | ‚ö°‚ö°‚ö° | Every query |
| **Reasoning** | FLAN-T5 | Generate responses | ‚ö° | Every query |
| **Safety** | Pattern-based | Validate safety | ‚ö°‚ö°‚ö° | Every query |
| **Execution** | None | Perform actions | ‚ö°‚ö°‚ö° | When tools needed |

---

## üîç **Perception Layer**
**Model:** `sentence-transformers/all-MiniLM-L6-v2` + `facebook/bart-large-mnli`

**What it does:**
- Converts text to 384-dim embeddings
- Classifies user intent (greeting, question, farewell)

**Example:**
```
Input: "What projects has Harith worked on?"
Output: {
  embedding: [0.123, -0.456, ...],
  intent: "QUESTION",
  confidence: 0.92
}
```

**Why specialized?**
- MiniLM is 10x faster than large models for embeddings
- BART-MNLI excels at zero-shot classification

---

## üóÑÔ∏è **Memory Layer**
**Model:** None (MongoDB Atlas Vector Search)

**What it does:**
- Searches for relevant portfolio/project data
- Stores conversation history

**Example:**
```
Input: embedding=[0.123, -0.456, ...]
Output: [
  {content: "SkinNet project...", score: 0.89},
  {content: "Computer vision expertise...", score: 0.84}
]
```

**Why specialized?**
- No AI overhead - pure database speed
- Optimized for vector similarity search

---

## üß† **Reasoning Layer**
**Model:** `google/flan-t5-large` (780M parameters)

**What it does:**
- Synthesizes context into coherent responses
- Follows intent-specific prompts

**Specialized Prompts:**

### For Greetings:
```
"Be brief, welcome user, introduce as Neo AI"
```

### For Questions (RAG):
```
"Synthesize information from knowledge base.
Speak ABOUT Harith in third person.
Be detailed and accurate."
```

### For Farewells:
```
"Be brief, thank them, leave good impression"
```

**Why specialized?**
- FLAN-T5 is instruction-tuned for following prompts
- Different prompts for different intents = better responses

---

## üîß **Execution Layer**
**Model:** None (pure execution logic)

**What it does:**
- Checks if projects are online
- Fetches GitHub statistics
- Performs calculations
- Gets current time

**Example:**
```
Input: action="check_project_status", params={project: "SkinNet"}
Output: {status: "online", url: "...", response_time: 123ms}
```

**Why specialized?**
- No AI needed for simple actions
- Faster and more reliable than LLM tool-calling

---

## üõ°Ô∏è **Safety Layer**
**Model:** Pattern-based filtering (optional: `toxic-bert`)

**What it does:**
- Validates input isn't malicious
- Checks output quality
- Rate limiting (60 req/min)

**Example:**
```
Input: "'; DROP TABLE users--"
Output: {is_safe: false, issues: ["SQL injection detected"]}
```

**Why specialized?**
- Pattern matching is instant (<5ms)
- Can add toxicity model later if needed

---

## üîÑ **Complete Flow Example**

```
User: "What AI projects has Harith built?"
  ‚Üì
Safety: ‚úì Input is safe
  ‚Üì
Perception: 
  ‚Ä¢ Embedding: [0.234, -0.123, ...]
  ‚Ä¢ Intent: QUESTION (92% confidence)
  ‚Üì
Memory:
  ‚Ä¢ Found: SkinNet, Object Detection, Neo AI
  ‚Ä¢ Scores: [0.89, 0.87, 0.85]
  ‚Üì
Reasoning (with specialized RAG prompt):
  "Harith Kavish has developed several AI projects:
   1. SkinNet-Analyzer - Deep learning for skin disease detection
   2. Multi-Object Detection using YOLO - Real-time object detection
   3. Neo AI - This intelligent portfolio assistant
   
   His work focuses on computer vision and deep learning..."
  ‚Üì
Safety: ‚úì Output is appropriate
  ‚Üì
Memory: Stored conversation for context
  ‚Üì
User receives response
```

---

## üí° **Key Benefits**

| Benefit | Before | After |
|---------|--------|-------|
| **Speed** | ~1500ms | ~960ms |
| **Accuracy** | Generic | Specialized |
| **Cost** | Higher | Lower |
| **Debugging** | Hard | Easy |
| **Scalability** | Limited | Modular |

---

## üöÄ **Next Steps**

### To Deploy Updates:
```bash
# Each layer needs to be redeployed to HuggingFace Spaces
cd spaces/perception-layer
git add .
git commit -m "Add specialized models and prompts"
git push

# Repeat for other layers
```

### To Test Locally:
```bash
# Test each layer individually
python spaces/perception-layer/app.py
python spaces/memory-layer/app.py
python spaces/reasoning-layer/app.py
```

### To Monitor Performance:
- Check startup logs for model loading
- Monitor response times per layer
- Track intent classification accuracy

---

## üìö **Related Documentation**

- Full details: `SPECIALIZED_ARCHITECTURE.md`
- Deployment: `DEPLOYMENT_GUIDE.md`
- Multi-agent overview: `MULTI_AGENT_ARCHITECTURE.md`

---

*Last Updated: November 6, 2025*
